{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ and PROCESS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=open(\"shakespear.txt\",\"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 99993 characters, 62 unique\n"
     ]
    }
   ],
   "source": [
    "chars= list(set(data))\n",
    "data_size, x_size= len(data), len(chars)\n",
    "print(f\"data has {data_size} characters, {X_size} unique\")\n",
    "char2idx= {ch:i for i ,ch in enumerate(chars)}\n",
    "idx2char= {i:ch for i ,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 100 #Size of Hidden layer \n",
    "T_steps = 25 # Time steps \n",
    "lr = 1e-1 \n",
    "std = 0.1 #standard deviation for weight initialization\n",
    "z_size = h_size + x_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions and it derivatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1-y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1-y*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zeros obtains memory from the operating system so that the OS zeroes it when it is first used. \\nzeros_like on the other hand fills the alloced memory with zeros by itself. B'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name \n",
    "        self.v = value #parameter value \n",
    "        self.d = np.zeros_like(value) #derivative \n",
    "        self.m = np.zeros_like(value) #adagrad memoentum \n",
    "\"\"\"zeros obtains memory from the operating system so that the OS zeroes it when it is first used. \n",
    "zeros_like on the other hand fills the alloced memory with zeros by itself. B\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use random weights with normal distribution (0, weight_sd) for  tanh  activation function \n",
    "#and (0.5,  weight_sd) for  sigmoid  activation function.\n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', np.random.randn(h_size, z_size) * std + 0.5)\n",
    "        self.b_f = Param('b_f', np.zeros((h_size, 1)))\n",
    "        self.W_i = Param('W_i', np.random.randn(h_size, z_size) * std + 0.5)\n",
    "        self.b_i = Param('b_i', np.zeros((h_size, 1)))\n",
    "        self.W_C = Param('W_C', np.random.randn(h_size, z_size) * std)\n",
    "        self.b_C = Param('b_C', np.zeros((h_size, 1)))\n",
    "        self.W_o = Param('W_o', np.random.randn(h_size, z_size) * std + 0.5)\n",
    "        self.b_o = Param('b_o', np.zeros((h_size, 1)))\n",
    "        #final prediction layer \n",
    "        self.W_v = Param('W_v', np.random.randn(x_size, h_size) * std  )\n",
    "        self.b_v = Param('b_v', np.zeros((x_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v, self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "\n",
    "parameters = Parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x,h_prev,C_prev,p=parameters):\n",
    "    assert x.shape() == (x_size, 1)\n",
    "    assert h_prev.shape() == (h_size, 1)\n",
    "    assert C_prev.shape() == (h_size, 1)\n",
    "    \n",
    "    z = np.vstack(h_prev,x)\n",
    "    f = sigmoid(p.W_f.v@z + p.b_f.v)\n",
    "    i = sigmoid(p.W_i.v@z + p.b_i.v)\n",
    "    C_bar = tanh(p.W_C.v@z + p.b_C.v)\n",
    "    o = sigmoid(p.W_o.v@z + p.b_o.v)\n",
    "    \n",
    "    C = C_prev*f + i*C_bar\n",
    "    \n",
    "    h = tanh(C)*o\n",
    "    \n",
    "    v = p.W_v.v@h + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax \n",
    "    \n",
    "    return z, f, i, o, C, C_bar, h, v, y\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(target, dh_next, dC_next, C_prev, z, f, i, o, C, C_bar, h, v, y, p=parameters):\n",
    "    \n",
    "    assert z.shape == (x_size + h_size, 1)\n",
    "    assert v.shape == (x_size, 1)\n",
    "    assert y.shape == (x_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (h_size, 1)\n",
    "    \n",
    "    #delta of v\n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "    #update W, b of v\n",
    "    p.W_v.d += dv @ h.T\n",
    "    p.b_v.d += dv\n",
    "    \n",
    "    #delta of h\n",
    "    dh = dh_next + p.W_v.T@dv\n",
    "    \n",
    "    #delta of o\n",
    "    do = dh * tanh(C) \n",
    "    do = dsigmoid(o) * do\n",
    "    #update W, b of o\n",
    "    p.W_o.d += do @ z.T\n",
    "    p.b_o.d += do\n",
    "    \n",
    "    #delta of C\n",
    "    dC = dC_next + dh*o*dtanh(tanh(C))\n",
    "   \n",
    "    #delta of C_bar\n",
    "    dC_bar = i * dC\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    #update W, b of C_bar\n",
    "    p.W_C.d += dC_bar @ z.T\n",
    "    p.b_C.d += dC_bar\n",
    "    \n",
    "    #delta of i \n",
    "    di = C_bar * dC\n",
    "    di = dsigmoid(i) * di\n",
    "    #update W, b of i\n",
    "    p.W_i.d += di @ z.T\n",
    "    p.b_i.d += di\n",
    "    \n",
    "    #delta of f\n",
    "    df = dC * C_prev \n",
    "    df = dsigmoid(f) * df\n",
    "    #update W, b of f\n",
    "    p.W_f.d += df @ z.T\n",
    "    p.b_f.d += df\n",
    "    \n",
    "    #delta of z\n",
    "    dz = p.W_f.v.T@df + p.W_C.v.T@dC_bar +\\\n",
    "         p.W_i.v.T@di + p.W_o.v.T@do\n",
    "    \n",
    "    dh_prev = dz[:h_size,:]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global parameters\n",
    "    #store the layers in each time step\n",
    "    #x, z, f, i, o, C, C_bar, h, v, y\n",
    "    \n",
    "    x_layer = {}\n",
    "    z_layer = {}\n",
    "    f_layer = {}\n",
    "    i_layer = {}\n",
    "    o_layer = {}\n",
    "    C_layer = {}\n",
    "    C_bar_layer = {}\n",
    "    h_layer = {}\n",
    "    v_layer = {}\n",
    "    y_layer = {}\n",
    "    \n",
    "    #set previous C, h\n",
    "    C_layer[-1] = np.copy(C_prev)\n",
    "    h_layer[-1] = np.copy(h_prev)\n",
    "    \n",
    "    loss = 0 \n",
    "    \n",
    "    assert len(inputs) = T_steps \n",
    "    \n",
    "    for t in range(T_steps):\n",
    "        #one hot \n",
    "        x_layer[t] = np.zeros((x_size, 1))\n",
    "        x_layer[t][target[t]] = 1\n",
    "        \n",
    "        #save the result of each layer\n",
    "        (z_layer[t], f_layer[t], i_layer[t], o_layer[t], \\\n",
    "         C_layer[t], C_bar_layer[t], h_layer[t], v_layer[t], y_layer[t])\\\n",
    "            = forward(x_layer[t], h_layer[t], C_layer[t], p=parameters)\n",
    "        \n",
    "        #accumulate loss\n",
    "        loss+= - np.log(y_layer[targets[t], 0]) #0 is for peel out the []\n",
    "    \n",
    "    \n",
    "    clear_gradients()\n",
    "    \n",
    "    dh_next = np.zeros_like(h_s[o])\n",
    "    dC_next = np.zeros_like(C_S[o])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dh_next, dC_next = back_prop(targets[t], dh_next, dC_next, C_layer[t-1], z_layer[t], f_layer[t], \\\n",
    "                           i_layer[t], o_layer[t], C_layer[t], C_bar_layer[t], h_layer[t], v_layer[t], y_layer[t])\n",
    "        \n",
    "        \n",
    "    clip_gradients()\n",
    "        \n",
    "    \n",
    "    return loss, h_layer[len(inputs)-1], C_layer[len(inputs) -1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c438a72fb923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "a[3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
